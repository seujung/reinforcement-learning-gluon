{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from mxboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrappers import make_atari, wrap_deepmind, wrap_mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env, frame_stack=True)\n",
    "env    = wrap_mxnet(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Block):\n",
    "    def __init__(self, input_shape, n_actions, **kwargs):\n",
    "        super(DuelingDQN, self).__init__(**kwargs)\n",
    "        \n",
    "\n",
    "        with self.name_scope():\n",
    "            self.conv1 = nn.Conv2D(32, 8, 4, in_channels=input_shape[0])\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv2 = nn.Conv2D(64, 4, 2, in_channels=32)\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            self.conv3 = nn.Conv2D(64, 3, 1, in_channels=64)\n",
    "            self.bn3 = nn.BatchNorm()\n",
    "            self.adv_layer = nn.Sequential()\n",
    "            self.adv_layer.add(nn.Dense(512, activation='relu'))\n",
    "            self.adv_layer.add(nn.Dense(n_actions, in_units=512))\n",
    "            self.value_layer = nn.Sequential()\n",
    "            self.value_layer.add(nn.Dense(512, activation='relu'))\n",
    "            self.value_layer.add(nn.Dense(1, in_units=512))\n",
    "#             self.advantage_fc1 = nn.Dense(512)\n",
    "#             self.advantage_fc2 = nn.Dense(n_actions, in_units=512)\n",
    "#             self.value_fc1 = nn.Dense(512)\n",
    "#             self.value_fc2 = nn.Dense(1, in_units=512)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = nd.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = nd.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = nd.relu(out)\n",
    "        out = nd.reshape(out, shape=(x.shape[0],-1))\n",
    "        adv_out = self.adv_layer(out)\n",
    "        value_out = self.value_layer(out)\n",
    "#         advantage = self.advantage_fc1(out)\n",
    "#         advantage = nd.relu(advantage)\n",
    "#         advantage = self.advantage_fc2(advantage)\n",
    "#         value = self.value_fc1(out)\n",
    "#         value = nd.relu(value)\n",
    "#         value = self.value_fc2(value)\n",
    "#         print(adv_out.shape)\n",
    "#         print(value_out.shape)\n",
    "        return value_out + adv_out - adv_out.mean()\n",
    "    \n",
    "    def act(self, state, epsilon, ctx):\n",
    "        if random.random() > epsilon:\n",
    "            state = nd.array(np.float32(state), ctx=ctx).expand_dims(0)\n",
    "            q_value = self.forward(state)\n",
    "            action = nd.argmax(q_value, axis=1)\n",
    "            action = int(action.asnumpy())\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = mx.random.uniform(shape=(1,4,84,84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net= DuelingDQN((4,84,84), 6)\n",
    "# net.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with autograd.record():\n",
    "#     pred= net(x)\n",
    "#     target = nd.ones(shape=(1,6))\n",
    "#     loss = pred - target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size, current_model, target_model, loss_fn, ctx):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = nd.array((np.float32(state)), ctx=ctx)\n",
    "    next_state = nd.array(np.float32(next_state), ctx=ctx)\n",
    "    action     = nd.array((action), ctx=ctx)\n",
    "    reward     = nd.array((reward), ctx=ctx)\n",
    "    done       = nd.array((done), ctx=ctx)\n",
    "   \n",
    "    q_values      = current_model(state)\n",
    "    next_q_values = current_model(next_state)\n",
    "    next_q_state_values = target_model(next_state) \n",
    "    \n",
    "    next_action = nd.argmax(next_q_values,1)\n",
    "    q_values = nd.gather_nd(q_values, nd.stack(nd.arange(action.shape[0], ctx=ctx).expand_dims(-1),action.expand_dims(-1), axis=0))\n",
    "    next_q_value = nd.gather_nd(next_q_state_values, nd.stack(nd.arange(next_action.shape[0], ctx=ctx).expand_dims(-1),\\\n",
    "                                                                  next_action.expand_dims(-1), axis=0))\n",
    "    q_values = q_values.squeeze()\n",
    "    next_q_value = next_q_value.squeeze()\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    loss = loss_fn(q_values, expected_q_value.detach())\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(current_model, target_model):\n",
    "    current_model.save_parameters('./tmp/dueliing_dqn_current_model')\n",
    "    target_model.load_parameters('./tmp/dueliing_dqn_current_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_initial = 10000\n",
    "replay_buffer = ReplayBuffer(100000)\n",
    "\n",
    "current_model = DuelingDQN(env.observation_space.shape, env.action_space.n)\n",
    "target_model = DuelingDQN(env.observation_space.shape, env.action_space.n)\n",
    "current_model.initialize(ctx=ctx)\n",
    "target_model.initialize(ctx=ctx)\n",
    "loss_fn = gluon.loss.L2Loss()\n",
    "trainer = gluon.Trainer(current_model.collect_params(), optimizer='adam', optimizer_params={'learning_rate':0.0001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 50000\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 1400000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "writer = SummaryWriter(logdir='./logs',filename_suffix=\"_DuelingDQN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "935: done 1 games, mean reward -21.000, reward -21.000, eps 0.98\n",
      "save current best model\n",
      "update target model\n",
      "1974: done 2 games, mean reward -20.000, reward -19.000, eps 0.96\n",
      "save current best model\n",
      "update target model\n",
      "2931: done 3 games, mean reward -20.000, reward -20.000, eps 0.94\n",
      "update target model\n",
      "3692: done 4 games, mean reward -20.250, reward -21.000, eps 0.93\n",
      "update target model\n",
      "4449: done 5 games, mean reward -20.400, reward -21.000, eps 0.92\n",
      "update target model\n",
      "5208: done 6 games, mean reward -20.500, reward -21.000, eps 0.90\n",
      "update target model\n",
      "6070: done 7 games, mean reward -20.429, reward -20.000, eps 0.89\n",
      "update target model\n",
      "7029: done 8 games, mean reward -20.500, reward -21.000, eps 0.87\n",
      "7788: done 9 games, mean reward -20.556, reward -21.000, eps 0.86\n",
      "update target model\n",
      "8701: done 10 games, mean reward -20.400, reward -19.000, eps 0.84\n",
      "update target model\n",
      "9458: done 11 games, mean reward -20.455, reward -21.000, eps 0.83\n",
      "update target model\n",
      "10234: done 12 games, mean reward -20.500, reward -21.000, eps 0.82\n",
      "update target model\n",
      "11161: done 13 games, mean reward -20.538, reward -21.000, eps 0.80\n",
      "update target model\n",
      "12111: done 14 games, mean reward -20.500, reward -20.000, eps 0.79\n",
      "12988: done 15 games, mean reward -20.533, reward -21.000, eps 0.77\n",
      "update target model\n",
      "13804: done 16 games, mean reward -20.562, reward -21.000, eps 0.76\n",
      "update target model\n",
      "14797: done 17 games, mean reward -20.529, reward -20.000, eps 0.75\n",
      "update target model\n",
      "15781: done 18 games, mean reward -20.556, reward -21.000, eps 0.73\n",
      "update target model\n",
      "16798: done 19 games, mean reward -20.526, reward -20.000, eps 0.72\n",
      "update target model\n",
      "17582: done 20 games, mean reward -20.550, reward -21.000, eps 0.71\n",
      "update target model\n",
      "18591: done 21 games, mean reward -20.524, reward -20.000, eps 0.69\n",
      "update target model\n",
      "19647: done 22 games, mean reward -20.455, reward -19.000, eps 0.68\n",
      "update target model\n",
      "20542: done 23 games, mean reward -20.435, reward -20.000, eps 0.67\n",
      "update target model\n",
      "21300: done 24 games, mean reward -20.458, reward -21.000, eps 0.66\n",
      "update target model\n",
      "22237: done 25 games, mean reward -20.400, reward -19.000, eps 0.64\n",
      "update target model\n",
      "23197: done 26 games, mean reward -20.385, reward -20.000, eps 0.63\n",
      "update target model\n",
      "24149: done 27 games, mean reward -20.370, reward -20.000, eps 0.62\n",
      "update target model\n",
      "25123: done 28 games, mean reward -20.393, reward -21.000, eps 0.61\n",
      "25958: done 29 games, mean reward -20.414, reward -21.000, eps 0.60\n",
      "update target model\n",
      "26792: done 30 games, mean reward -20.400, reward -20.000, eps 0.59\n",
      "update target model\n",
      "27640: done 31 games, mean reward -20.419, reward -21.000, eps 0.58\n",
      "update target model\n",
      "28729: done 32 games, mean reward -20.375, reward -19.000, eps 0.57\n",
      "update target model\n",
      "29731: done 33 games, mean reward -20.364, reward -20.000, eps 0.56\n",
      "update target model\n",
      "update target model\n",
      "31079: done 34 games, mean reward -20.265, reward -17.000, eps 0.54\n",
      "update target model\n",
      "32158: done 35 games, mean reward -20.257, reward -20.000, eps 0.53\n",
      "update target model\n",
      "33121: done 36 games, mean reward -20.278, reward -21.000, eps 0.52\n",
      "update target model\n",
      "34135: done 37 games, mean reward -20.243, reward -19.000, eps 0.51\n",
      "update target model\n",
      "35034: done 38 games, mean reward -20.237, reward -20.000, eps 0.50\n",
      "35913: done 39 games, mean reward -20.256, reward -21.000, eps 0.49\n",
      "update target model\n",
      "36945: done 40 games, mean reward -20.250, reward -20.000, eps 0.48\n",
      "update target model\n",
      "37781: done 41 games, mean reward -20.244, reward -20.000, eps 0.48\n",
      "update target model\n",
      "38856: done 42 games, mean reward -20.214, reward -19.000, eps 0.47\n",
      "update target model\n",
      "39755: done 43 games, mean reward -20.233, reward -21.000, eps 0.46\n",
      "update target model\n",
      "40714: done 44 games, mean reward -20.250, reward -21.000, eps 0.45\n",
      "update target model\n",
      "41923: done 45 games, mean reward -20.244, reward -20.000, eps 0.44\n",
      "update target model\n",
      "42927: done 46 games, mean reward -20.261, reward -21.000, eps 0.43\n",
      "update target model\n",
      "43917: done 47 games, mean reward -20.277, reward -21.000, eps 0.42\n",
      "update target model\n",
      "update target model\n",
      "45047: done 48 games, mean reward -20.271, reward -20.000, eps 0.41\n",
      "update target model\n",
      "46194: done 49 games, mean reward -20.224, reward -18.000, eps 0.40\n",
      "update target model\n",
      "47357: done 50 games, mean reward -20.220, reward -20.000, eps 0.39\n",
      "update target model\n",
      "48638: done 51 games, mean reward -20.196, reward -19.000, eps 0.38\n",
      "update target model\n",
      "49619: done 52 games, mean reward -20.212, reward -21.000, eps 0.38\n",
      "update target model\n",
      "50803: done 53 games, mean reward -20.208, reward -20.000, eps 0.37\n",
      "update target model\n",
      "51776: done 54 games, mean reward -20.185, reward -19.000, eps 0.36\n",
      "update target model\n",
      "update target model\n",
      "53068: done 55 games, mean reward -20.182, reward -20.000, eps 0.35\n",
      "update target model\n",
      "54315: done 56 games, mean reward -20.161, reward -19.000, eps 0.34\n",
      "update target model\n",
      "55570: done 57 games, mean reward -20.123, reward -18.000, eps 0.34\n",
      "update target model\n",
      "update target model\n",
      "57174: done 58 games, mean reward -20.103, reward -19.000, eps 0.33\n",
      "update target model\n",
      "58119: done 59 games, mean reward -20.102, reward -20.000, eps 0.32\n",
      "update target model\n",
      "59453: done 60 games, mean reward -20.083, reward -19.000, eps 0.31\n",
      "update target model\n",
      "60825: done 61 games, mean reward -20.082, reward -20.000, eps 0.30\n",
      "update target model\n",
      "update target model\n",
      "62273: done 62 games, mean reward -20.081, reward -20.000, eps 0.29\n",
      "update target model\n",
      "63426: done 63 games, mean reward -20.063, reward -19.000, eps 0.29\n",
      "update target model\n",
      "update target model\n",
      "65120: done 64 games, mean reward -20.016, reward -17.000, eps 0.28\n",
      "update target model\n",
      "66726: done 65 games, mean reward -19.969, reward -17.000, eps 0.27\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "68194: done 66 games, mean reward -19.939, reward -18.000, eps 0.26\n",
      "save current best model\n",
      "update target model\n",
      "69857: done 67 games, mean reward -19.866, reward -15.000, eps 0.25\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "71250: done 68 games, mean reward -19.868, reward -20.000, eps 0.25\n",
      "update target model\n",
      "72474: done 69 games, mean reward -19.841, reward -18.000, eps 0.24\n",
      "save current best model\n",
      "update target model\n",
      "73771: done 70 games, mean reward -19.829, reward -19.000, eps 0.24\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "75344: done 71 games, mean reward -19.803, reward -18.000, eps 0.23\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "77073: done 72 games, mean reward -19.750, reward -16.000, eps 0.22\n",
      "save current best model\n",
      "update target model\n",
      "78874: done 73 games, mean reward -19.685, reward -15.000, eps 0.21\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "80447: done 74 games, mean reward -19.649, reward -17.000, eps 0.21\n",
      "save current best model\n",
      "update target model\n",
      "81780: done 75 games, mean reward -19.627, reward -18.000, eps 0.20\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "83399: done 76 games, mean reward -19.566, reward -15.000, eps 0.20\n",
      "save current best model\n",
      "update target model\n",
      "84859: done 77 games, mean reward -19.545, reward -18.000, eps 0.19\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "86089: done 78 games, mean reward -19.538, reward -19.000, eps 0.19\n",
      "save current best model\n",
      "update target model\n",
      "87566: done 79 games, mean reward -19.494, reward -16.000, eps 0.18\n",
      "save current best model\n",
      "update target model\n",
      "88775: done 80 games, mean reward -19.475, reward -18.000, eps 0.18\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "90245: done 81 games, mean reward -19.444, reward -17.000, eps 0.17\n",
      "save current best model\n",
      "update target model\n",
      "91663: done 82 games, mean reward -19.439, reward -19.000, eps 0.17\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "93362: done 83 games, mean reward -19.398, reward -16.000, eps 0.16\n",
      "save current best model\n",
      "update target model\n",
      "94687: done 84 games, mean reward -19.405, reward -20.000, eps 0.16\n",
      "update target model\n",
      "update target model\n",
      "96364: done 85 games, mean reward -19.353, reward -15.000, eps 0.15\n",
      "save current best model\n",
      "update target model\n",
      "97928: done 86 games, mean reward -19.349, reward -19.000, eps 0.15\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "99419: done 87 games, mean reward -19.333, reward -18.000, eps 0.15\n",
      "save current best model\n",
      "update target model\n",
      "100844: done 88 games, mean reward -19.307, reward -17.000, eps 0.14\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "102603: done 89 games, mean reward -19.281, reward -17.000, eps 0.14\n",
      "save current best model\n",
      "update target model\n",
      "103960: done 90 games, mean reward -19.267, reward -18.000, eps 0.13\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "105574: done 91 games, mean reward -19.253, reward -18.000, eps 0.13\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "107179: done 92 games, mean reward -19.228, reward -17.000, eps 0.13\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "109039: done 93 games, mean reward -19.194, reward -16.000, eps 0.12\n",
      "save current best model\n",
      "update target model\n",
      "110816: done 94 games, mean reward -19.149, reward -15.000, eps 0.12\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "112456: done 95 games, mean reward -19.116, reward -16.000, eps 0.11\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "114189: done 96 games, mean reward -19.062, reward -14.000, eps 0.11\n",
      "save current best model\n",
      "update target model\n",
      "115802: done 97 games, mean reward -19.052, reward -18.000, eps 0.11\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "117412: done 98 games, mean reward -19.031, reward -17.000, eps 0.10\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "119027: done 99 games, mean reward -19.030, reward -19.000, eps 0.10\n",
      "save current best model\n",
      "update target model\n",
      "120863: done 100 games, mean reward -18.970, reward -13.000, eps 0.10\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "122587: done 101 games, mean reward -18.930, reward -17.000, eps 0.10\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "124109: done 102 games, mean reward -18.930, reward -19.000, eps 0.09\n",
      "update target model\n",
      "125850: done 103 games, mean reward -18.890, reward -16.000, eps 0.09\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "127624: done 104 games, mean reward -18.840, reward -16.000, eps 0.09\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "129994: done 105 games, mean reward -18.750, reward -12.000, eps 0.08\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "131950: done 106 games, mean reward -18.700, reward -16.000, eps 0.08\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "133766: done 107 games, mean reward -18.630, reward -13.000, eps 0.08\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "135686: done 108 games, mean reward -18.560, reward -14.000, eps 0.08\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "137442: done 109 games, mean reward -18.520, reward -17.000, eps 0.07\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "139247: done 110 games, mean reward -18.450, reward -12.000, eps 0.07\n",
      "save current best model\n",
      "update target model\n",
      "140597: done 111 games, mean reward -18.410, reward -17.000, eps 0.07\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "142947: done 112 games, mean reward -18.320, reward -12.000, eps 0.07\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "144851: done 113 games, mean reward -18.240, reward -13.000, eps 0.06\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "146444: done 114 games, mean reward -18.210, reward -17.000, eps 0.06\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "148827: done 115 games, mean reward -18.130, reward -13.000, eps 0.06\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "150890: done 116 games, mean reward -18.060, reward -14.000, eps 0.06\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "152942: done 117 games, mean reward -17.980, reward -12.000, eps 0.06\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "154459: done 118 games, mean reward -17.940, reward -17.000, eps 0.06\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "156436: done 119 games, mean reward -17.890, reward -15.000, eps 0.05\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "158357: done 120 games, mean reward -17.810, reward -13.000, eps 0.05\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "160775: done 121 games, mean reward -17.700, reward -9.000, eps 0.05\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "163472: done 122 games, mean reward -17.560, reward -5.000, eps 0.05\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "165749: done 123 games, mean reward -17.450, reward -9.000, eps 0.05\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "167726: done 124 games, mean reward -17.370, reward -13.000, eps 0.04\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "169842: done 125 games, mean reward -17.280, reward -10.000, eps 0.04\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "172048: done 126 games, mean reward -17.200, reward -12.000, eps 0.04\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "174090: done 127 games, mean reward -17.130, reward -13.000, eps 0.04\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "176250: done 128 games, mean reward -17.000, reward -8.000, eps 0.04\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "178496: done 129 games, mean reward -16.900, reward -11.000, eps 0.04\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "180844: done 130 games, mean reward -16.790, reward -9.000, eps 0.04\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "183222: done 131 games, mean reward -16.690, reward -11.000, eps 0.04\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "185340: done 132 games, mean reward -16.620, reward -12.000, eps 0.03\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "188019: done 133 games, mean reward -16.470, reward -5.000, eps 0.03\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "190519: done 134 games, mean reward -16.370, reward -7.000, eps 0.03\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "192974: done 135 games, mean reward -16.230, reward -6.000, eps 0.03\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "195412: done 136 games, mean reward -16.120, reward -10.000, eps 0.03\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "197802: done 137 games, mean reward -16.020, reward -9.000, eps 0.03\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "200083: done 138 games, mean reward -15.920, reward -10.000, eps 0.03\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "202417: done 139 games, mean reward -15.790, reward -8.000, eps 0.03\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "204972: done 140 games, mean reward -15.650, reward -6.000, eps 0.03\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "207292: done 141 games, mean reward -15.540, reward -9.000, eps 0.03\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "210059: done 142 games, mean reward -15.390, reward -4.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "211951: done 143 games, mean reward -15.310, reward -13.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "214689: done 144 games, mean reward -15.170, reward -7.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "217081: done 145 games, mean reward -15.050, reward -8.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "219498: done 146 games, mean reward -14.950, reward -11.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "221229: done 147 games, mean reward -14.880, reward -14.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "224044: done 148 games, mean reward -14.740, reward -6.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "227022: done 149 games, mean reward -14.600, reward -4.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "229168: done 150 games, mean reward -14.500, reward -10.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "231576: done 151 games, mean reward -14.430, reward -12.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "233717: done 152 games, mean reward -14.350, reward -13.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "236427: done 153 games, mean reward -14.220, reward -7.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "238884: done 154 games, mean reward -14.120, reward -9.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "241547: done 155 games, mean reward -14.010, reward -9.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "244214: done 156 games, mean reward -13.880, reward -6.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "246842: done 157 games, mean reward -13.780, reward -8.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "249288: done 158 games, mean reward -13.680, reward -9.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "251524: done 159 games, mean reward -13.570, reward -9.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "253406: done 160 games, mean reward -13.480, reward -10.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "255712: done 161 games, mean reward -13.390, reward -11.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "258197: done 162 games, mean reward -13.260, reward -7.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "260343: done 163 games, mean reward -13.190, reward -12.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "262875: done 164 games, mean reward -13.090, reward -7.000, eps 0.02\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "265057: done 165 games, mean reward -13.010, reward -9.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "267012: done 166 games, mean reward -12.950, reward -12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "269467: done 167 games, mean reward -12.860, reward -6.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "271990: done 168 games, mean reward -12.720, reward -6.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "273915: done 169 games, mean reward -12.650, reward -11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "276065: done 170 games, mean reward -12.560, reward -10.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "279014: done 171 games, mean reward -12.420, reward -4.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "281213: done 172 games, mean reward -12.330, reward -7.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "283264: done 173 games, mean reward -12.270, reward -9.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "285683: done 174 games, mean reward -12.170, reward -7.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "287754: done 175 games, mean reward -12.090, reward -10.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "290174: done 176 games, mean reward -12.010, reward -7.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "292854: done 177 games, mean reward -11.870, reward -4.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "295865: done 178 games, mean reward -11.730, reward -5.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "298540: done 179 games, mean reward -11.630, reward -6.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "301344: done 180 games, mean reward -11.500, reward -5.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "304332: done 181 games, mean reward -11.360, reward -3.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "307679: done 182 games, mean reward -11.160, reward 1.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "310354: done 183 games, mean reward -11.040, reward -4.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "312246: done 184 games, mean reward -10.960, reward -12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "314845: done 185 games, mean reward -10.850, reward -4.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "317390: done 186 games, mean reward -10.700, reward -4.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "320488: done 187 games, mean reward -10.540, reward -2.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "322413: done 188 games, mean reward -10.480, reward -11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "324671: done 189 games, mean reward -10.400, reward -9.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "327457: done 190 games, mean reward -10.260, reward -4.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "330473: done 191 games, mean reward -10.100, reward -2.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "332476: done 192 games, mean reward -10.030, reward -10.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "335301: done 193 games, mean reward -9.880, reward -1.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "337603: done 194 games, mean reward -9.790, reward -6.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "340402: done 195 games, mean reward -9.680, reward -5.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "342610: done 196 games, mean reward -9.620, reward -8.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "345355: done 197 games, mean reward -9.480, reward -4.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "347928: done 198 games, mean reward -9.360, reward -5.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "350787: done 199 games, mean reward -9.160, reward 1.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "353479: done 200 games, mean reward -8.960, reward 7.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "356023: done 201 games, mean reward -8.840, reward -5.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "359092: done 202 games, mean reward -8.640, reward 1.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "361873: done 203 games, mean reward -8.510, reward -3.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "365278: done 204 games, mean reward -8.340, reward 1.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "367676: done 205 games, mean reward -8.120, reward 10.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "370598: done 206 games, mean reward -7.940, reward 2.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "373546: done 207 games, mean reward -7.800, reward 1.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "376162: done 208 games, mean reward -7.730, reward -7.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "379063: done 209 games, mean reward -7.590, reward -3.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "382087: done 210 games, mean reward -7.450, reward 2.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "384856: done 211 games, mean reward -7.220, reward 6.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "387310: done 212 games, mean reward -7.020, reward 8.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "390077: done 213 games, mean reward -6.910, reward -2.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "392997: done 214 games, mean reward -6.700, reward 4.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "395612: done 215 games, mean reward -6.510, reward 6.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "397675: done 216 games, mean reward -6.220, reward 15.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "400625: done 217 games, mean reward -6.080, reward 2.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "403404: done 218 games, mean reward -5.860, reward 5.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "406203: done 219 games, mean reward -5.660, reward 5.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "409411: done 220 games, mean reward -5.520, reward 1.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "412216: done 221 games, mean reward -5.400, reward 3.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "414995: done 222 games, mean reward -5.390, reward -4.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "417911: done 223 games, mean reward -5.280, reward 2.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "420422: done 224 games, mean reward -5.200, reward -5.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "423194: done 225 games, mean reward -5.130, reward -3.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "425764: done 226 games, mean reward -4.950, reward 6.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "427694: done 227 games, mean reward -4.930, reward -11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "430351: done 228 games, mean reward -4.800, reward 5.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "433011: done 229 games, mean reward -4.630, reward 6.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "435679: done 230 games, mean reward -4.460, reward 8.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "438189: done 231 games, mean reward -4.240, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "440733: done 232 games, mean reward -4.030, reward 9.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "442773: done 233 games, mean reward -3.820, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "445840: done 234 games, mean reward -3.730, reward 2.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "448821: done 235 games, mean reward -3.660, reward 1.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "451400: done 236 games, mean reward -3.490, reward 7.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "454266: done 237 games, mean reward -3.380, reward 2.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "457069: done 238 games, mean reward -3.200, reward 8.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "459324: done 239 games, mean reward -3.000, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "461722: done 240 games, mean reward -2.820, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "464018: done 241 games, mean reward -2.650, reward 8.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "466403: done 242 games, mean reward -2.490, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "469415: done 243 games, mean reward -2.260, reward 10.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "471467: done 244 games, mean reward -2.000, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "473707: done 245 games, mean reward -1.760, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "476357: done 246 games, mean reward -1.560, reward 9.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "479117: done 247 games, mean reward -1.340, reward 8.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "481668: done 248 games, mean reward -1.180, reward 10.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "484297: done 249 games, mean reward -1.080, reward 6.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "486889: done 250 games, mean reward -0.890, reward 9.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "489134: done 251 games, mean reward -0.650, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "491342: done 252 games, mean reward -0.360, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "493901: done 253 games, mean reward -0.230, reward 6.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "496744: done 254 games, mean reward -0.070, reward 7.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "499520: done 255 games, mean reward 0.040, reward 2.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "502124: done 256 games, mean reward 0.200, reward 10.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "504446: done 257 games, mean reward 0.360, reward 8.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "506735: done 258 games, mean reward 0.560, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "509139: done 259 games, mean reward 0.740, reward 9.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "511552: done 260 games, mean reward 0.970, reward 13.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "513694: done 261 games, mean reward 1.250, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "516139: done 262 games, mean reward 1.430, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "518389: done 263 games, mean reward 1.690, reward 14.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "520730: done 264 games, mean reward 1.870, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "523223: done 265 games, mean reward 2.040, reward 8.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "525404: done 266 games, mean reward 2.320, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "528114: done 267 games, mean reward 2.460, reward 8.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "530010: done 268 games, mean reward 2.680, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "532458: done 269 games, mean reward 2.920, reward 13.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "535125: done 270 games, mean reward 3.100, reward 8.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "537617: done 271 games, mean reward 3.210, reward 7.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "540233: done 272 games, mean reward 3.370, reward 9.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "542467: done 273 games, mean reward 3.580, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "544940: done 274 games, mean reward 3.770, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "547655: done 275 games, mean reward 3.940, reward 7.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "550301: done 276 games, mean reward 4.090, reward 8.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "552495: done 277 games, mean reward 4.280, reward 15.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "554782: done 278 games, mean reward 4.440, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "557288: done 279 games, mean reward 4.580, reward 8.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "559586: done 280 games, mean reward 4.750, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "562089: done 281 games, mean reward 4.920, reward 14.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "564813: done 282 games, mean reward 4.960, reward 5.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "567151: done 283 games, mean reward 5.140, reward 14.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "569624: done 284 games, mean reward 5.350, reward 9.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "571698: done 285 games, mean reward 5.550, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "574028: done 286 games, mean reward 5.700, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "576189: done 287 games, mean reward 5.840, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "578877: done 288 games, mean reward 6.020, reward 7.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "580974: done 289 games, mean reward 6.260, reward 15.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "583174: done 290 games, mean reward 6.430, reward 13.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "586107: done 291 games, mean reward 6.490, reward 4.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "588561: done 292 games, mean reward 6.730, reward 14.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "590648: done 293 games, mean reward 6.910, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "592835: done 294 games, mean reward 7.090, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "594942: done 295 games, mean reward 7.320, reward 18.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "597044: done 296 games, mean reward 7.570, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "599393: done 297 games, mean reward 7.720, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "601647: done 298 games, mean reward 7.940, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "603791: done 299 games, mean reward 8.100, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "606292: done 300 games, mean reward 8.160, reward 13.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "608635: done 301 games, mean reward 8.330, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "611472: done 302 games, mean reward 8.410, reward 9.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "614105: done 303 games, mean reward 8.510, reward 7.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "616631: done 304 games, mean reward 8.560, reward 6.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "618937: done 305 games, mean reward 8.580, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "621314: done 306 games, mean reward 8.670, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "623819: done 307 games, mean reward 8.730, reward 7.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "625890: done 308 games, mean reward 8.960, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "627869: done 309 games, mean reward 9.150, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "630033: done 310 games, mean reward 9.300, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "632290: done 311 games, mean reward 9.360, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "634674: done 312 games, mean reward 9.380, reward 10.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "636842: done 313 games, mean reward 9.540, reward 14.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "638907: done 314 games, mean reward 9.660, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "640993: done 315 games, mean reward 9.750, reward 15.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "643054: done 316 games, mean reward 9.770, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "645118: done 317 games, mean reward 9.910, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "647154: done 318 games, mean reward 10.030, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "649553: done 319 games, mean reward 10.080, reward 10.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "651594: done 320 games, mean reward 10.210, reward 14.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "654048: done 321 games, mean reward 10.290, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "656075: done 322 games, mean reward 10.480, reward 15.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "658471: done 323 games, mean reward 10.570, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "660805: done 324 games, mean reward 10.730, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "662989: done 325 games, mean reward 10.880, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "665780: done 326 games, mean reward 10.880, reward 6.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "668298: done 327 games, mean reward 11.100, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "671018: done 328 games, mean reward 11.150, reward 10.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "673672: done 329 games, mean reward 11.150, reward 6.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "675710: done 330 games, mean reward 11.240, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "677941: done 331 games, mean reward 11.270, reward 14.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "680754: done 332 games, mean reward 11.290, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "682991: done 333 games, mean reward 11.280, reward 15.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "685550: done 334 games, mean reward 11.310, reward 5.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "687587: done 335 games, mean reward 11.460, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "689503: done 336 games, mean reward 11.560, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "691390: done 337 games, mean reward 11.710, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "693778: done 338 games, mean reward 11.730, reward 10.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "696191: done 339 games, mean reward 11.730, reward 12.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "698814: done 340 games, mean reward 11.720, reward 11.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "700933: done 341 games, mean reward 11.800, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "703004: done 342 games, mean reward 11.840, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "705045: done 343 games, mean reward 11.920, reward 18.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "707120: done 344 games, mean reward 11.890, reward 16.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "709134: done 345 games, mean reward 11.910, reward 18.000, eps 0.01\n",
      "update target model\n",
      "710769: done 346 games, mean reward 12.030, reward 21.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "712776: done 347 games, mean reward 12.110, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "715471: done 348 games, mean reward 12.060, reward 5.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "717648: done 349 games, mean reward 12.160, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "719589: done 350 games, mean reward 12.240, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "721528: done 351 games, mean reward 12.310, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "723801: done 352 games, mean reward 12.300, reward 15.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "725664: done 353 games, mean reward 12.430, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "727777: done 354 games, mean reward 12.530, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "729766: done 355 games, mean reward 12.700, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "732008: done 356 games, mean reward 12.730, reward 13.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "734151: done 357 games, mean reward 12.810, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "736334: done 358 games, mean reward 12.870, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "738435: done 359 games, mean reward 12.930, reward 15.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "740622: done 360 games, mean reward 12.950, reward 15.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "742583: done 361 games, mean reward 12.930, reward 15.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "744447: done 362 games, mean reward 13.010, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "746293: done 363 games, mean reward 13.060, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "748009: done 364 games, mean reward 13.140, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "750353: done 365 games, mean reward 13.170, reward 11.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "752390: done 366 games, mean reward 13.170, reward 16.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "754376: done 367 games, mean reward 13.250, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "756234: done 368 games, mean reward 13.270, reward 18.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "759338: done 369 games, mean reward 13.220, reward 8.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "761957: done 370 games, mean reward 13.240, reward 10.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "764393: done 371 games, mean reward 13.290, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "766606: done 372 games, mean reward 13.320, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "768724: done 373 games, mean reward 13.340, reward 14.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "770998: done 374 games, mean reward 13.330, reward 11.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "772969: done 375 games, mean reward 13.450, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "775129: done 376 games, mean reward 13.540, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "777047: done 377 games, mean reward 13.580, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "779039: done 378 games, mean reward 13.620, reward 15.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "781200: done 379 games, mean reward 13.710, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "783119: done 380 games, mean reward 13.760, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "785214: done 381 games, mean reward 13.760, reward 14.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "787263: done 382 games, mean reward 13.870, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "790080: done 383 games, mean reward 13.780, reward 5.000, eps 0.01\n",
      "update target model\n",
      "791989: done 384 games, mean reward 13.870, reward 18.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "794031: done 385 games, mean reward 13.870, reward 16.000, eps 0.01\n",
      "update target model\n",
      "795783: done 386 games, mean reward 13.950, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "797941: done 387 games, mean reward 13.950, reward 12.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "799982: done 388 games, mean reward 14.050, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "802057: done 389 games, mean reward 14.070, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "804203: done 390 games, mean reward 14.090, reward 15.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "806217: done 391 games, mean reward 14.220, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "808329: done 392 games, mean reward 14.240, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "810847: done 393 games, mean reward 14.170, reward 10.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "812807: done 394 games, mean reward 14.230, reward 18.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "814650: done 395 games, mean reward 14.230, reward 18.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "816563: done 396 games, mean reward 14.250, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "819154: done 397 games, mean reward 14.210, reward 7.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "821229: done 398 games, mean reward 14.180, reward 14.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "823087: done 399 games, mean reward 14.200, reward 19.000, eps 0.01\n",
      "update target model\n",
      "824894: done 400 games, mean reward 14.260, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "826583: done 401 games, mean reward 14.330, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "828543: done 402 games, mean reward 14.400, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "831118: done 403 games, mean reward 14.450, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "833532: done 404 games, mean reward 14.510, reward 12.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "835389: done 405 games, mean reward 14.580, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "837216: done 406 games, mean reward 14.640, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "840355: done 407 games, mean reward 14.630, reward 6.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "842368: done 408 games, mean reward 14.630, reward 16.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "844729: done 409 games, mean reward 14.620, reward 15.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "846503: done 410 games, mean reward 14.640, reward 19.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "848609: done 411 games, mean reward 14.710, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "850686: done 412 games, mean reward 14.780, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "853136: done 413 games, mean reward 14.740, reward 10.000, eps 0.01\n",
      "update target model\n",
      "854930: done 414 games, mean reward 14.780, reward 20.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "857683: done 415 games, mean reward 14.700, reward 7.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "859749: done 416 games, mean reward 14.710, reward 18.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "861830: done 417 games, mean reward 14.720, reward 17.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "864012: done 418 games, mean reward 14.680, reward 13.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "866667: done 419 games, mean reward 14.630, reward 5.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "868636: done 420 games, mean reward 14.680, reward 19.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "870738: done 421 games, mean reward 14.720, reward 15.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "872523: done 422 games, mean reward 14.780, reward 21.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "874696: done 423 games, mean reward 14.810, reward 14.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "876780: done 424 games, mean reward 14.870, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "878866: done 425 games, mean reward 14.880, reward 13.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "881025: done 426 games, mean reward 14.980, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "883014: done 427 games, mean reward 15.050, reward 18.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "885238: done 428 games, mean reward 15.110, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "886956: done 429 games, mean reward 15.260, reward 21.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "888918: done 430 games, mean reward 15.260, reward 17.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "891232: done 431 games, mean reward 15.230, reward 11.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "893200: done 432 games, mean reward 15.280, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "894992: done 433 games, mean reward 15.330, reward 20.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "897882: done 434 games, mean reward 15.360, reward 8.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "899958: done 435 games, mean reward 15.370, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "901678: done 436 games, mean reward 15.410, reward 21.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "903816: done 437 games, mean reward 15.410, reward 17.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "905623: done 438 games, mean reward 15.480, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "907678: done 439 games, mean reward 15.510, reward 15.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "909684: done 440 games, mean reward 15.580, reward 18.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "911896: done 441 games, mean reward 15.590, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "914182: done 442 games, mean reward 15.560, reward 13.000, eps 0.01\n",
      "update target model\n",
      "915959: done 443 games, mean reward 15.580, reward 20.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "918237: done 444 games, mean reward 15.560, reward 14.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "920176: done 445 games, mean reward 15.560, reward 18.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "922037: done 446 games, mean reward 15.530, reward 18.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "924323: done 447 games, mean reward 15.510, reward 14.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "926306: done 448 games, mean reward 15.630, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "928769: done 449 games, mean reward 15.580, reward 11.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "930721: done 450 games, mean reward 15.590, reward 18.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "933020: done 451 games, mean reward 15.520, reward 12.000, eps 0.01\n",
      "update target model\n",
      "934801: done 452 games, mean reward 15.560, reward 19.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "936473: done 453 games, mean reward 15.580, reward 21.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "938585: done 454 games, mean reward 15.580, reward 17.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "940207: done 455 games, mean reward 15.600, reward 21.000, eps 0.01\n",
      "update target model\n",
      "941925: done 456 games, mean reward 15.670, reward 20.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "944532: done 457 games, mean reward 15.620, reward 11.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "946553: done 458 games, mean reward 15.630, reward 18.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "948726: done 459 games, mean reward 15.640, reward 16.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "950627: done 460 games, mean reward 15.660, reward 17.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "952682: done 461 games, mean reward 15.680, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "954556: done 462 games, mean reward 15.670, reward 18.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "956568: done 463 games, mean reward 15.650, reward 17.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "958848: done 464 games, mean reward 15.600, reward 14.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "960836: done 465 games, mean reward 15.670, reward 18.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "962757: done 466 games, mean reward 15.690, reward 18.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "964903: done 467 games, mean reward 15.690, reward 16.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "967103: done 468 games, mean reward 15.670, reward 16.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "969161: done 469 games, mean reward 15.750, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "971258: done 470 games, mean reward 15.790, reward 14.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "973445: done 471 games, mean reward 15.820, reward 15.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "975216: done 472 games, mean reward 15.900, reward 20.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "976987: done 473 games, mean reward 15.940, reward 18.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "978865: done 474 games, mean reward 16.000, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "980748: done 475 games, mean reward 15.990, reward 18.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "983074: done 476 games, mean reward 15.960, reward 14.000, eps 0.01\n",
      "update target model\n",
      "984810: done 477 games, mean reward 15.970, reward 20.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "986717: done 478 games, mean reward 16.020, reward 20.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "989324: done 479 games, mean reward 15.900, reward 5.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "991220: done 480 games, mean reward 15.910, reward 18.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "993154: done 481 games, mean reward 15.930, reward 16.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "995143: done 482 games, mean reward 15.940, reward 17.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "997085: done 483 games, mean reward 16.050, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "998942: done 484 games, mean reward 16.060, reward 19.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "1000981: done 485 games, mean reward 16.070, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "1002997: done 486 games, mean reward 16.070, reward 19.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "update target model\n",
      "1005276: done 487 games, mean reward 16.070, reward 12.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "1007278: done 488 games, mean reward 16.060, reward 16.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "1009933: done 489 games, mean reward 16.010, reward 12.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "1011917: done 490 games, mean reward 16.000, reward 14.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "1013813: done 491 games, mean reward 16.020, reward 19.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "1015777: done 492 games, mean reward 16.050, reward 19.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "1017508: done 493 games, mean reward 16.150, reward 20.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "1019281: done 494 games, mean reward 16.170, reward 20.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "1021451: done 495 games, mean reward 16.150, reward 16.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "1023290: done 496 games, mean reward 16.160, reward 20.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "1025370: done 497 games, mean reward 16.260, reward 17.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "1027236: done 498 games, mean reward 16.300, reward 18.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "1029407: done 499 games, mean reward 16.280, reward 17.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "1031338: done 500 games, mean reward 16.250, reward 16.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "1033632: done 501 games, mean reward 16.200, reward 14.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "1035319: done 502 games, mean reward 16.250, reward 21.000, eps 0.01\n",
      "update target model\n",
      "update target model\n",
      "1037021: done 503 games, mean reward 16.340, reward 21.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n",
      "update target model\n",
      "1039128: done 504 games, mean reward 16.380, reward 16.000, eps 0.01\n",
      "save current best model\n",
      "update target model\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "current_best = -100\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = current_model.act(state, epsilon, ctx)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        writer.add_scalar(\"reward\", episode_reward, frame_idx)  \n",
    "        mean_reward = np.mean(all_rewards[-100:])\n",
    "        print(\"%d: done %d games, mean reward %.3f, reward %.3f, eps %.2f\" % (\n",
    "                frame_idx, len(all_rewards), mean_reward, episode_reward, epsilon,\n",
    "            ))\n",
    "        if current_best < mean_reward:\n",
    "            print(\"save current best model\")\n",
    "            current_model.save_parameters('./models/double_dqn_best_model')\n",
    "            current_best = mean_reward\n",
    "        episode_reward = 0\n",
    "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "        writer.add_scalar(\"mean_reward\", mean_reward, frame_idx)  \n",
    "        \n",
    "        \n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        with autograd.record():\n",
    "            loss = compute_td_loss(batch_size, current_model, target_model, loss_fn, ctx)\n",
    "            loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        losses.append(loss.sum().asscalar())\n",
    "        writer.add_scalar(\"loss\", loss.mean().asscalar(), frame_idx)   \n",
    "    if frame_idx % 1000 == 0:\n",
    "        print(\"update target model\")\n",
    "        update_target(current_model, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    state      = nd.array((np.float32(state)), ctx=ctx)\n",
    "    next_state = nd.array(np.float32(next_state), ctx=ctx)\n",
    "    action     = nd.array(action, ctx=ctx)\n",
    "    reward     = nd.array(reward, ctx=ctx)\n",
    "    done       = nd.array(done, ctx=ctx)\n",
    "   \n",
    "    q_values      = current_model(state)\n",
    "    next_q_values = current_model(next_state)\n",
    "    next_q_state_values = target_model(next_state)\n",
    "    \n",
    "    next_action = nd.argmax(next_q_values,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "<NDArray 32 @gpu(0)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[4. 4. 4. 4. 4. 4. 1. 4. 4. 4. 1. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
       " 4. 4. 4. 4. 4. 4. 1. 4.]\n",
       "<NDArray 32 @gpu(0)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.argmax(next_q_values,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = nd.gather_nd(q_values, nd.stack(nd.arange(action.shape[0], ctx=ctx).expand_dims(-1),action.expand_dims(-1), axis=0))\n",
    "next_q_value = nd.gather_nd(next_q_state_values, nd.stack(nd.arange(next_action.shape[0], ctx=ctx).expand_dims(-1),\\\n",
    "                                                                  next_action.expand_dims(-1), axis=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_q_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = q_values.squeeze()\n",
    "next_q_value = next_q_value.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_q_value = reward + gamma * next_q_value * (1 - done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[7.77847767e-01 8.75926670e-03 9.11522985e-01 1.61150675e-02\n",
       " 1.02776647e+00 5.36763556e-02 3.00979435e-01 7.47589052e-01\n",
       " 1.31033516e+00 1.39697935e-04 6.04455243e-04 5.77379346e-01\n",
       " 1.18786907e+00 1.15350652e+00 1.82130409e-03 3.74929160e-01\n",
       " 9.89390373e-01 6.70911074e-01 1.20459557e+00 7.89059550e-02\n",
       " 1.23624317e-02 6.92399263e-01 3.42928439e-01 1.04627311e+00\n",
       " 8.34418647e-03 3.46169651e-01 1.58085692e+00 7.51657188e-01\n",
       " 2.93571725e-02 1.33847715e-02 5.35753276e-03 8.88727009e-01]\n",
       "<NDArray 32 @gpu(0)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(q_values,expected_q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_q_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = gamma * next_q_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
