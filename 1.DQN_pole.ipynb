{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from mxboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from IPython.display import clear_output\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrappers import make_atari, wrap_deepmind, wrap_mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v0\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Block):\n",
    "    def __init__(self, input_shape, n_actions, **kwargs):\n",
    "        super(DQN, self).__init__(**kwargs)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.fc1 = nn.Dense(128)\n",
    "            self.fc2 = nn.Dense(n_actions, in_units=128)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = nd.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    def act(self, state, epsilon, ctx):\n",
    "        if random.random() > epsilon:\n",
    "            state = nd.array(np.float32(state), ctx=ctx).expand_dims(0) \n",
    "            q_value = self.forward(state)\n",
    "            action = nd.argmax(q_value, axis=1)\n",
    "            action = int(action.asnumpy())\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size, net, loss_fn, ctx):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = nd.array((np.float32(state)), ctx=ctx) \n",
    "    next_state = nd.array(np.float32(next_state), ctx=ctx)\n",
    "    action     = nd.array((action), ctx=ctx)\n",
    "    reward     = nd.array((reward), ctx=ctx)\n",
    "    done       = nd.array((done), ctx=ctx)\n",
    "\n",
    "    q_values      = net(state)\n",
    "    next_q_values = net(next_state)\n",
    "    \n",
    "    q_values = nd.gather_nd(q_values, nd.stack(nd.arange(action.shape[0], ctx=ctx).expand_dims(-1),action.expand_dims(-1), axis=0))\n",
    "    next_q_value     = next_q_values.max(1)\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = loss_fn(q_values, expected_q_value)\n",
    "#     loss = nd.power(q_values - expected_q_value,2).sum()\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_initial = 100\n",
    "replay_buffer = ReplayBuffer(200)\n",
    "\n",
    "net = DQN(env.observation_space.shape, env.action_space.n)\n",
    "net.initialize(ctx=ctx)\n",
    "loss_fn = gluon.loss.L2Loss()\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer='adam', optimizer_params={'learning_rate':0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 20000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "writer = SummaryWriter(logdir='./logs',filename_suffix=\"_Cart_Pole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27: done 1 games, mean reward 27.000, episode reward 27.000,  eps 0.95\n",
      "save current best model\n",
      "57: done 2 games, mean reward 28.500, episode reward 30.000,  eps 0.89\n",
      "save current best model\n",
      "71: done 3 games, mean reward 23.667, episode reward 14.000,  eps 0.87\n",
      "82: done 4 games, mean reward 20.500, episode reward 11.000,  eps 0.85\n",
      "106: done 5 games, mean reward 21.200, episode reward 24.000,  eps 0.81\n",
      "121: done 6 games, mean reward 20.167, episode reward 15.000,  eps 0.79\n",
      "136: done 7 games, mean reward 19.429, episode reward 15.000,  eps 0.76\n",
      "162: done 8 games, mean reward 20.250, episode reward 26.000,  eps 0.73\n",
      "181: done 9 games, mean reward 20.111, episode reward 19.000,  eps 0.70\n",
      "218: done 10 games, mean reward 21.800, episode reward 37.000,  eps 0.65\n",
      "241: done 11 games, mean reward 21.909, episode reward 23.000,  eps 0.62\n",
      "266: done 12 games, mean reward 22.167, episode reward 25.000,  eps 0.59\n",
      "284: done 13 games, mean reward 21.846, episode reward 18.000,  eps 0.57\n",
      "362: done 14 games, mean reward 25.857, episode reward 78.000,  eps 0.49\n",
      "374: done 15 games, mean reward 24.933, episode reward 12.000,  eps 0.48\n",
      "390: done 16 games, mean reward 24.375, episode reward 16.000,  eps 0.46\n",
      "416: done 17 games, mean reward 24.471, episode reward 26.000,  eps 0.44\n",
      "455: done 18 games, mean reward 25.278, episode reward 39.000,  eps 0.41\n",
      "476: done 19 games, mean reward 25.053, episode reward 21.000,  eps 0.39\n",
      "497: done 20 games, mean reward 24.850, episode reward 21.000,  eps 0.38\n",
      "554: done 21 games, mean reward 26.381, episode reward 57.000,  eps 0.34\n",
      "600: done 22 games, mean reward 27.273, episode reward 46.000,  eps 0.31\n",
      "633: done 23 games, mean reward 27.522, episode reward 33.000,  eps 0.29\n",
      "676: done 24 games, mean reward 28.167, episode reward 43.000,  eps 0.27\n",
      "755: done 25 games, mean reward 30.200, episode reward 79.000,  eps 0.23\n",
      "save current best model\n",
      "778: done 26 games, mean reward 29.923, episode reward 23.000,  eps 0.22\n",
      "816: done 27 games, mean reward 30.222, episode reward 38.000,  eps 0.20\n",
      "save current best model\n",
      "887: done 28 games, mean reward 31.679, episode reward 71.000,  eps 0.18\n",
      "save current best model\n",
      "931: done 29 games, mean reward 32.103, episode reward 44.000,  eps 0.16\n",
      "save current best model\n",
      "959: done 30 games, mean reward 31.967, episode reward 28.000,  eps 0.16\n",
      "1009: done 31 games, mean reward 32.548, episode reward 50.000,  eps 0.14\n",
      "save current best model\n",
      "1102: done 32 games, mean reward 34.438, episode reward 93.000,  eps 0.12\n",
      "save current best model\n",
      "1160: done 33 games, mean reward 35.152, episode reward 58.000,  eps 0.11\n",
      "save current best model\n",
      "1252: done 34 games, mean reward 36.824, episode reward 92.000,  eps 0.09\n",
      "save current best model\n",
      "1363: done 35 games, mean reward 38.943, episode reward 111.000,  eps 0.07\n",
      "save current best model\n",
      "1483: done 36 games, mean reward 41.194, episode reward 120.000,  eps 0.06\n",
      "save current best model\n",
      "1508: done 37 games, mean reward 40.757, episode reward 25.000,  eps 0.06\n",
      "1521: done 38 games, mean reward 40.026, episode reward 13.000,  eps 0.06\n",
      "1721: done 39 games, mean reward 44.128, episode reward 200.000,  eps 0.04\n",
      "save current best model\n",
      "1921: done 40 games, mean reward 48.025, episode reward 200.000,  eps 0.03\n",
      "save current best model\n",
      "2121: done 41 games, mean reward 51.732, episode reward 200.000,  eps 0.02\n",
      "save current best model\n",
      "2321: done 42 games, mean reward 55.262, episode reward 200.000,  eps 0.02\n",
      "save current best model\n",
      "2521: done 43 games, mean reward 58.628, episode reward 200.000,  eps 0.02\n",
      "save current best model\n",
      "2721: done 44 games, mean reward 61.841, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "2912: done 45 games, mean reward 64.711, episode reward 191.000,  eps 0.01\n",
      "save current best model\n",
      "2984: done 46 games, mean reward 64.870, episode reward 72.000,  eps 0.01\n",
      "save current best model\n",
      "3000: done 47 games, mean reward 63.830, episode reward 16.000,  eps 0.01\n",
      "3051: done 48 games, mean reward 63.562, episode reward 51.000,  eps 0.01\n",
      "3202: done 49 games, mean reward 65.347, episode reward 151.000,  eps 0.01\n",
      "save current best model\n",
      "3402: done 50 games, mean reward 68.040, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "3602: done 51 games, mean reward 70.627, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "3802: done 52 games, mean reward 73.115, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "3928: done 53 games, mean reward 74.113, episode reward 126.000,  eps 0.01\n",
      "save current best model\n",
      "4042: done 54 games, mean reward 74.852, episode reward 114.000,  eps 0.01\n",
      "save current best model\n",
      "4077: done 55 games, mean reward 74.127, episode reward 35.000,  eps 0.01\n",
      "4277: done 56 games, mean reward 76.375, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "4477: done 57 games, mean reward 78.544, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "4611: done 58 games, mean reward 79.500, episode reward 134.000,  eps 0.01\n",
      "save current best model\n",
      "4685: done 59 games, mean reward 79.407, episode reward 74.000,  eps 0.01\n",
      "4813: done 60 games, mean reward 80.217, episode reward 128.000,  eps 0.01\n",
      "save current best model\n",
      "5013: done 61 games, mean reward 82.180, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "5213: done 62 games, mean reward 84.081, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "5413: done 63 games, mean reward 85.921, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "5613: done 64 games, mean reward 87.703, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "5813: done 65 games, mean reward 89.431, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "6013: done 66 games, mean reward 91.106, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "6213: done 67 games, mean reward 92.731, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "6413: done 68 games, mean reward 94.309, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "6613: done 69 games, mean reward 95.841, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "6813: done 70 games, mean reward 97.329, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "7008: done 71 games, mean reward 98.704, episode reward 195.000,  eps 0.01\n",
      "save current best model\n",
      "7131: done 72 games, mean reward 99.042, episode reward 123.000,  eps 0.01\n",
      "save current best model\n",
      "7278: done 73 games, mean reward 99.699, episode reward 147.000,  eps 0.01\n",
      "save current best model\n",
      "7400: done 74 games, mean reward 100.000, episode reward 122.000,  eps 0.01\n",
      "save current best model\n",
      "7600: done 75 games, mean reward 101.333, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "7800: done 76 games, mean reward 102.632, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "8000: done 77 games, mean reward 103.896, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "8200: done 78 games, mean reward 105.128, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "8400: done 79 games, mean reward 106.329, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "8600: done 80 games, mean reward 107.500, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "8800: done 81 games, mean reward 108.642, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "8939: done 82 games, mean reward 109.012, episode reward 139.000,  eps 0.01\n",
      "save current best model\n",
      "9132: done 83 games, mean reward 110.024, episode reward 193.000,  eps 0.01\n",
      "save current best model\n",
      "9332: done 84 games, mean reward 111.095, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "9532: done 85 games, mean reward 112.141, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "9732: done 86 games, mean reward 113.163, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "9932: done 87 games, mean reward 114.161, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "9944: done 88 games, mean reward 113.000, episode reward 12.000,  eps 0.01\n",
      "10144: done 89 games, mean reward 113.978, episode reward 200.000,  eps 0.01\n",
      "10155: done 90 games, mean reward 112.833, episode reward 11.000,  eps 0.01\n",
      "10355: done 91 games, mean reward 113.791, episode reward 200.000,  eps 0.01\n",
      "10555: done 92 games, mean reward 114.728, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "10755: done 93 games, mean reward 115.645, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "10955: done 94 games, mean reward 116.543, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "11155: done 95 games, mean reward 117.421, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "11355: done 96 games, mean reward 118.281, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "11399: done 97 games, mean reward 117.515, episode reward 44.000,  eps 0.01\n",
      "11410: done 98 games, mean reward 116.429, episode reward 11.000,  eps 0.01\n",
      "11423: done 99 games, mean reward 115.384, episode reward 13.000,  eps 0.01\n",
      "11478: done 100 games, mean reward 114.780, episode reward 55.000,  eps 0.01\n",
      "11678: done 101 games, mean reward 116.510, episode reward 200.000,  eps 0.01\n",
      "11878: done 102 games, mean reward 118.210, episode reward 200.000,  eps 0.01\n",
      "12078: done 103 games, mean reward 120.070, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "12114: done 104 games, mean reward 120.320, episode reward 36.000,  eps 0.01\n",
      "save current best model\n",
      "12126: done 105 games, mean reward 120.200, episode reward 12.000,  eps 0.01\n",
      "12284: done 106 games, mean reward 121.630, episode reward 158.000,  eps 0.01\n",
      "save current best model\n",
      "12452: done 107 games, mean reward 123.160, episode reward 168.000,  eps 0.01\n",
      "save current best model\n",
      "12652: done 108 games, mean reward 124.900, episode reward 200.000,  eps 0.01\n",
      "save current best model\n",
      "12809: done 109 games, mean reward 126.280, episode reward 157.000,  eps 0.01\n",
      "save current best model\n",
      "12948: done 110 games, mean reward 127.300, episode reward 139.000,  eps 0.01\n",
      "save current best model\n",
      "13099: done 111 games, mean reward 128.580, episode reward 151.000,  eps 0.01\n",
      "save current best model\n",
      "13168: done 112 games, mean reward 129.020, episode reward 69.000,  eps 0.01\n",
      "save current best model\n",
      "13294: done 113 games, mean reward 130.100, episode reward 126.000,  eps 0.01\n",
      "save current best model\n",
      "13448: done 114 games, mean reward 130.860, episode reward 154.000,  eps 0.01\n",
      "save current best model\n",
      "13618: done 115 games, mean reward 132.440, episode reward 170.000,  eps 0.01\n",
      "save current best model\n",
      "13774: done 116 games, mean reward 133.840, episode reward 156.000,  eps 0.01\n",
      "save current best model\n",
      "13828: done 117 games, mean reward 134.120, episode reward 54.000,  eps 0.01\n",
      "save current best model\n",
      "14000: done 118 games, mean reward 135.450, episode reward 172.000,  eps 0.01\n",
      "save current best model\n",
      "14104: done 119 games, mean reward 136.280, episode reward 104.000,  eps 0.01\n",
      "save current best model\n",
      "14218: done 120 games, mean reward 137.210, episode reward 114.000,  eps 0.01\n",
      "save current best model\n",
      "14248: done 121 games, mean reward 136.940, episode reward 30.000,  eps 0.01\n",
      "14289: done 122 games, mean reward 136.890, episode reward 41.000,  eps 0.01\n",
      "14357: done 123 games, mean reward 137.240, episode reward 68.000,  eps 0.01\n",
      "save current best model\n",
      "14425: done 124 games, mean reward 137.490, episode reward 68.000,  eps 0.01\n",
      "save current best model\n",
      "14486: done 125 games, mean reward 137.310, episode reward 61.000,  eps 0.01\n",
      "14549: done 126 games, mean reward 137.710, episode reward 63.000,  eps 0.01\n",
      "save current best model\n",
      "14614: done 127 games, mean reward 137.980, episode reward 65.000,  eps 0.01\n",
      "save current best model\n",
      "14673: done 128 games, mean reward 137.860, episode reward 59.000,  eps 0.01\n",
      "14727: done 129 games, mean reward 137.960, episode reward 54.000,  eps 0.01\n",
      "14796: done 130 games, mean reward 138.370, episode reward 69.000,  eps 0.01\n",
      "save current best model\n",
      "14865: done 131 games, mean reward 138.560, episode reward 69.000,  eps 0.01\n",
      "save current best model\n",
      "14936: done 132 games, mean reward 138.340, episode reward 71.000,  eps 0.01\n",
      "15004: done 133 games, mean reward 138.440, episode reward 68.000,  eps 0.01\n",
      "15063: done 134 games, mean reward 138.110, episode reward 59.000,  eps 0.01\n",
      "15129: done 135 games, mean reward 137.660, episode reward 66.000,  eps 0.01\n",
      "15184: done 136 games, mean reward 137.010, episode reward 55.000,  eps 0.01\n",
      "15252: done 137 games, mean reward 137.440, episode reward 68.000,  eps 0.01\n",
      "15324: done 138 games, mean reward 138.030, episode reward 72.000,  eps 0.01\n",
      "15379: done 139 games, mean reward 136.580, episode reward 55.000,  eps 0.01\n",
      "15446: done 140 games, mean reward 135.250, episode reward 67.000,  eps 0.01\n",
      "15458: done 141 games, mean reward 133.370, episode reward 12.000,  eps 0.01\n",
      "15509: done 142 games, mean reward 131.880, episode reward 51.000,  eps 0.01\n",
      "15575: done 143 games, mean reward 130.540, episode reward 66.000,  eps 0.01\n",
      "15646: done 144 games, mean reward 129.250, episode reward 71.000,  eps 0.01\n",
      "15715: done 145 games, mean reward 128.030, episode reward 69.000,  eps 0.01\n",
      "15798: done 146 games, mean reward 128.140, episode reward 83.000,  eps 0.01\n",
      "15878: done 147 games, mean reward 128.780, episode reward 80.000,  eps 0.01\n",
      "15957: done 148 games, mean reward 129.060, episode reward 79.000,  eps 0.01\n",
      "16034: done 149 games, mean reward 128.320, episode reward 77.000,  eps 0.01\n",
      "16118: done 150 games, mean reward 127.160, episode reward 84.000,  eps 0.01\n",
      "16224: done 151 games, mean reward 126.220, episode reward 106.000,  eps 0.01\n",
      "16313: done 152 games, mean reward 125.110, episode reward 89.000,  eps 0.01\n",
      "16404: done 153 games, mean reward 124.760, episode reward 91.000,  eps 0.01\n",
      "16604: done 154 games, mean reward 125.620, episode reward 200.000,  eps 0.01\n",
      "16675: done 155 games, mean reward 125.980, episode reward 71.000,  eps 0.01\n",
      "16875: done 156 games, mean reward 125.980, episode reward 200.000,  eps 0.01\n",
      "17075: done 157 games, mean reward 125.980, episode reward 200.000,  eps 0.01\n",
      "17275: done 158 games, mean reward 126.640, episode reward 200.000,  eps 0.01\n",
      "17475: done 159 games, mean reward 127.900, episode reward 200.000,  eps 0.01\n",
      "17675: done 160 games, mean reward 128.620, episode reward 200.000,  eps 0.01\n",
      "17875: done 161 games, mean reward 128.620, episode reward 200.000,  eps 0.01\n",
      "18075: done 162 games, mean reward 128.620, episode reward 200.000,  eps 0.01\n",
      "18275: done 163 games, mean reward 128.620, episode reward 200.000,  eps 0.01\n",
      "18475: done 164 games, mean reward 128.620, episode reward 200.000,  eps 0.01\n",
      "18675: done 165 games, mean reward 128.620, episode reward 200.000,  eps 0.01\n",
      "18875: done 166 games, mean reward 128.620, episode reward 200.000,  eps 0.01\n",
      "19075: done 167 games, mean reward 128.620, episode reward 200.000,  eps 0.01\n",
      "19275: done 168 games, mean reward 128.620, episode reward 200.000,  eps 0.01\n",
      "19475: done 169 games, mean reward 128.620, episode reward 200.000,  eps 0.01\n",
      "19675: done 170 games, mean reward 128.620, episode reward 200.000,  eps 0.01\n",
      "19875: done 171 games, mean reward 128.670, episode reward 200.000,  eps 0.01\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "current_best = 0.0\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = net.act(state, epsilon, ctx)\n",
    "#     print(action)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        writer.add_scalar(\"reward\", episode_reward, frame_idx)  \n",
    "        mean_reward = np.mean(all_rewards[-100:])\n",
    "        print(\"%d: done %d games, mean reward %.3f, episode reward %.3f,  eps %.2f\" % (\n",
    "                frame_idx, len(all_rewards), mean_reward, episode_reward, epsilon,\n",
    "            ))\n",
    "        if current_best < mean_reward:\n",
    "            print(\"save current best model\")\n",
    "            net.save_parameters('./models/cartpole_best_model')\n",
    "            current_best = mean_reward\n",
    "        episode_reward = 0\n",
    "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "        writer.add_scalar(\"mean_reward\", mean_reward, frame_idx)  \n",
    "        \n",
    "        \n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        with autograd.record():\n",
    "#             print(\"compute loss\")\n",
    "            loss = compute_td_loss(batch_size, net, loss_fn, ctx)\n",
    "            loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        losses.append(loss.sum().asscalar())\n",
    "        writer.add_scalar(\"loss\", loss.mean().asscalar(), frame_idx) \n",
    "#     if frame_idx % 10000 == 0:\n",
    "#         plot(frame_idx, all_rewards, losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
